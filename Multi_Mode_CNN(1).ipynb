{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla T4\n",
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "# Import librairies\n",
    "import jupyter_client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['PROJ_LIB'] = '/home/roussillon/bin/miniconda3/envs/mon_env_py3/lib/python3.7/site-packages/mpl_toolkits/basemap'\n",
    "#from eof import eof, seasonal_sig, interannual_sig, get_wgts, detrend\n",
    "#from analysis import get_gen_chl, get_var, rmse_chl, nrmse_chl, print_map, cross_correlation\n",
    "from mpl_toolkits.basemap import Basemap,cm \n",
    "import glob\n",
    "from scipy.signal.windows import hann\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import gridspec\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import xarray as xr\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import cv2 as cv\n",
    "import argparse\n",
    "from torchvision import models, transforms\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolated_Img_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_folder, input_file, output_file , transform=None, normalize=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_folder (String): path to input and output files\n",
    "            input_file (String): npy file of the input data to be used by the NN\n",
    "            output_file (String): npy file of the output data to be use by the NN\n",
    "            transform (callable, Optional): Optional transform to be applied on\n",
    "            a sample\n",
    "        \"\"\"\n",
    "        self.root_folder = root_folder\n",
    "        self.input_arr = np.load(os.path.join(self.root_folder, input_file ))\n",
    "        self.output_arr = np.load(os.path.join(self.root_folder, output_file))\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "        self.mean_input = np.mean(self.input_arr, axis=(0,1,2))\n",
    "        self.std_input = np.std(self.input_arr, axis=(0,1,2))\n",
    "        self.mean_output = np.mean(self.output_arr)\n",
    "        self.std_output = np.std(self.output_arr)        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.input_arr.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.input_arr[idx,...]\n",
    "        Y = self.output_arr[idx,...]\n",
    "        \n",
    "        if self.normalize:\n",
    "            for ch in range(13):\n",
    "                X[:,ch] = (X[:,ch] - self.mean_input[ch])/ self.std_input[ch]   ################### ch ??? 13 ??\n",
    "            Y = (Y - self.mean_output)/ self.std_output\n",
    "            \n",
    "        if self.transform:\n",
    "            X =  self.transform(X)\n",
    "            Y =  self.transform(Y)\n",
    "        \n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 2\n",
    "\n",
    "# convert data to a torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = './'\n",
    "\n",
    "# Load input and reference data from .npy files :\n",
    "train_data = Interpolated_Img_Dataset(img_path, 'normalized_input_physical_data_1998_2015.npy', 'ln_Chl_ref_norm_1998_2015.npy', transform=transform, normalize=False) \n",
    "\n",
    "# Load mean and std of normalized logarithm Chl reference data :\n",
    "mean_Y = np.load('mean_ln_chl_norm.npy')\n",
    "std_Y = np.load('std_ln_chl_norm.npy')\n",
    "\n",
    "# Define train, validation and test datasets : \n",
    "train_idx, valid_idx_1, valid_idx_2 = [i for i in range(60,156)], [i for i in range(0,48)], [i for i in range(168,216)] \n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx) \n",
    "valid_sampler_1 = SubsetRandomSampler(valid_idx_1)\n",
    "valid_sampler_2 = SubsetRandomSampler(valid_idx_2)\n",
    "\n",
    "# convert to data loaders :\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader_1 = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler_1, num_workers=num_workers)\n",
    "valid_loader_2 = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler_2, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the model architecture and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition of 8 sub-models Mi with the architecture :\n",
      "CNN_M1(\n",
      "  (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.35, inplace=False)\n",
      ")\n",
      "and one attention module W that outputs 8 weighted maps :\n",
      "CNN_W(\n",
      "  (conv1): Conv2d(9, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (dropout): Dropout(p=0.35, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Definition of the 8 sub-models : \n",
    "\n",
    "class CNN_M1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNN_M2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M4, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M6, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M7, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_M8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_M8, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,64,3, padding =(1,1))\n",
    "        self.conv4 = nn.Conv2d(64,128,3, padding =(1,1))\n",
    "        self.conv5 = nn.Conv2d(128,1,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "    \n",
    "# Definition of the W attention module :\n",
    "    \n",
    "class CNN_W(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_W, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(9,16,3, padding =(1,1))    \n",
    "        self.conv2 = nn.Conv2d(16,32,3, padding =(1,1))\n",
    "        self.conv3 = nn.Conv2d(32,8,3, padding =(1,1))\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = (F.softmax(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "model_M1 = CNN_M1()\n",
    "model_M2 = CNN_M2()\n",
    "model_M3 = CNN_M3()\n",
    "model_M4 = CNN_M4()\n",
    "model_M5 = CNN_M5()\n",
    "model_M6 = CNN_M6()\n",
    "model_M7 = CNN_M7()\n",
    "model_M8 = CNN_M8()\n",
    "model_W = CNN_W()\n",
    "\n",
    "# move tensors to GPU if CUDA is available\n",
    "model_M1.to(torch.device('cuda:0'))         \n",
    "model_M1 = model_M1.double()\n",
    "             \n",
    "model_M2.to(torch.device('cuda:0'))         \n",
    "model_M2 = model_M2.double()\n",
    "\n",
    "model_M3.to(torch.device('cuda:0'))         \n",
    "model_M3 = model_M3.double()\n",
    "\n",
    "model_M4.to(torch.device('cuda:0'))         \n",
    "model_M4 = model_M4.double()\n",
    "\n",
    "model_M5.to(torch.device('cuda:0'))         \n",
    "model_M5 = model_M5.double()\n",
    "\n",
    "model_M6.to(torch.device('cuda:0'))         \n",
    "model_M6 = model_M6.double()\n",
    "\n",
    "model_M7.to(torch.device('cuda:0'))         \n",
    "model_M7 = model_M7.double()\n",
    "\n",
    "model_M8.to(torch.device('cuda:0'))         \n",
    "model_M8 = model_M8.double()\n",
    "             \n",
    "model_W.to(torch.device('cuda:0'))         \n",
    "model_W = model_W.double()\n",
    "\n",
    "# specify loss function \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer_1 = optim.Adam(model_M1.parameters(), lr=0.001)\n",
    "optimizer_2 = optim.Adam(model_M2.parameters(), lr=0.001)\n",
    "optimizer_3 = optim.Adam(model_M3.parameters(), lr=0.001)\n",
    "optimizer_4 = optim.Adam(model_M4.parameters(), lr=0.001)\n",
    "optimizer_5 = optim.Adam(model_M5.parameters(), lr=0.001)\n",
    "optimizer_6 = optim.Adam(model_M6.parameters(), lr=0.001)\n",
    "optimizer_7 = optim.Adam(model_M7.parameters(), lr=0.001)\n",
    "optimizer_8 = optim.Adam(model_M8.parameters(), lr=0.001)\n",
    "optimizer_W = optim.Adam(model_W.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "print('Definition of 8 sub-models Mi with the architecture :') \n",
    "print(model_M1)\n",
    "print('and one attention module W that outputs 8 weighted maps :')\n",
    "print(model_W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarbonTracker: The following components were found: GPU with device(s) Tesla T4. CPU with device(s) .\n",
      "Epoch: 1/5 \tTraining Loss: 0.232770 \tValidation Loss 1: 0.151104 \tValidation Loss 2: 0.205393\n",
      "CarbonTracker: Live carbon intensity could not be fetched at detected location: Brest, Brittany, FR. Defaulted to average carbon intensity for EU-28 in 2017 of 294.21 gCO2/kWh.\n",
      "CarbonTracker: \n",
      "Predicted consumption for 6 epoch(s):\n",
      "\tTime:\t0:28:14\n",
      "\tEnergy:\t0.041784 kWh\n",
      "\tCO2eq:\t12.293000 g\n",
      "\tThis is equivalent to:\n",
      "\t0.102101 km travelled by car\n",
      "Epoch: 2/5 \tTraining Loss: 0.151635 \tValidation Loss 1: 0.127553 \tValidation Loss 2: 0.178409\n",
      "Epoch: 3/5 \tTraining Loss: 0.128931 \tValidation Loss 1: 0.110687 \tValidation Loss 2: 0.173397\n",
      "Epoch: 4/5 \tTraining Loss: 0.119371 \tValidation Loss 1: 0.103005 \tValidation Loss 2: 0.163804\n",
      "Epoch: 5/5 \tTraining Loss: 0.107748 \tValidation Loss 1: 0.099748 \tValidation Loss 2: 0.156609\n",
      "CarbonTracker: Average carbon intensity during training was 294.21 gCO2/kWh at detected location: Brest, Brittany, FR.\n",
      "CarbonTracker: \n",
      "Actual consumption for 5 epoch(s):\n",
      "\tTime:\t0:23:30\n",
      "\tEnergy:\t0.036453 kWh\n",
      "\tCO2eq:\t10.724608 g\n",
      "\tThis is equivalent to:\n",
      "\t0.089075 km travelled by car\n",
      "CarbonTracker: Finished monitoring.\n",
      "Time computation : 30.34 min\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "######### Weighted neural network  ##########  #### 8 modes\n",
    "#############################################\n",
    "\n",
    "model_1 = model_M1   \n",
    "model_2 = model_M2  \n",
    "model_3 = model_M3  \n",
    "model_4 = model_M4 \n",
    "model_5 = model_M5 \n",
    "model_6 = model_M6 \n",
    "model_7 = model_M7 \n",
    "model_8 = model_M8 \n",
    "model_W = model_W \n",
    "\n",
    "\n",
    "## Specify number of epochs : \n",
    "n_epochs = 500\n",
    "\n",
    "## Specify a folder name in which the trained odels will be saved, as well as their root name :\n",
    "folder_name = './FOLDER_TO_SAVE_TRAINED_MODELS/'\n",
    "model_name = 'root_names_of_trained_modeds'\n",
    "\n",
    "folder_name2 = folder_name + model_name + '/'\n",
    "model_save_1 = folder_name + model_name + '_1.pt'\n",
    "model_save_2 = folder_name + model_name + '_2.pt'\n",
    "model_save_3 = folder_name + model_name + '_3.pt'\n",
    "model_save_4 = folder_name + model_name + '_4.pt'\n",
    "model_save_5 = folder_name + model_name + '_5.pt'\n",
    "model_save_6 = folder_name + model_name + '_6.pt'\n",
    "model_save_7 = folder_name + model_name + '_7.pt'\n",
    "model_save_8 = folder_name + model_name + '_8.pt'\n",
    "model_save_W = folder_name + model_name + '_W.pt'\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "# Monitoring metrics during training loop :\n",
    "loss_mask = 0\n",
    "corr_epoque_affichage = 10\n",
    "scatter_epoque_affichage = 50\n",
    "loss_fig = 10\n",
    "loss_fig_epoque_affichage = 10\n",
    "write_file_epoque = 10\n",
    "scatter_corr = 1\n",
    "\n",
    "#######################\n",
    "# Training loop :\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Import continental mask to compute the loss only on the ocean (not on the land)\n",
    "mask = torch.load('inter_mask.pt')\n",
    "mask = mask.bool()\n",
    "mask = mask.reshape([1,1,178,358])\n",
    "mask.to(device)\n",
    "\n",
    "# Monitoring carbon footprint\n",
    "import time\n",
    "from carbontracker.tracker import CarbonTracker\n",
    "from carbontracker import parser\n",
    "\n",
    "# Check for time computation\n",
    "tps_ini = time.clock()\n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create output folder to save the data\n",
    "if not os.path.exists(folder_name2):\n",
    " os.makedirs(folder_name2)\n",
    "\n",
    "tracker = CarbonTracker(epochs=n_epochs+1, verbose = 2, log_dir = folder_name2, monitor_epochs = n_epochs)\n",
    "\n",
    "# Initialisation of the loss value\n",
    "train_losses_save, test_losses_save_1, test_losses_save_2 = [], [], [] \n",
    "\n",
    "valid_loss_min_1 = np.Inf # track change in validation loss\n",
    "valid_loss_min_2 = np.Inf # track change in validation loss\n",
    "\n",
    "scheduler1 = torch.optim.lr_scheduler.MultiStepLR(optimizer_1, milestones=[400], gamma=0.1)\n",
    "scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer_2, milestones=[400], gamma=0.1)\n",
    "scheduler3 = torch.optim.lr_scheduler.MultiStepLR(optimizer_3, milestones=[400], gamma=0.1)\n",
    "scheduler4 = torch.optim.lr_scheduler.MultiStepLR(optimizer_4, milestones=[400], gamma=0.1)\n",
    "scheduler5 = torch.optim.lr_scheduler.MultiStepLR(optimizer_5, milestones=[400], gamma=0.1)\n",
    "scheduler6 = torch.optim.lr_scheduler.MultiStepLR(optimizer_6, milestones=[400], gamma=0.1)\n",
    "scheduler7 = torch.optim.lr_scheduler.MultiStepLR(optimizer_7, milestones=[400], gamma=0.1)\n",
    "scheduler8 = torch.optim.lr_scheduler.MultiStepLR(optimizer_8, milestones=[400], gamma=0.1)\n",
    "schedulerW = torch.optim.lr_scheduler.MultiStepLR(optimizer_W, milestones=[400], gamma=0.1)\n",
    "\n",
    "from carbontracker import parser   \n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    tracker.epoch_start()\n",
    "    \n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss_1 = 0.0\n",
    "    valid_loss_2 = 0.0\n",
    "    \n",
    "    compteur = 0\n",
    "        \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "        \n",
    "    model_1.train()\n",
    "    model_2.train()\n",
    "    model_3.train()\n",
    "    model_4.train()\n",
    "    model_5.train()\n",
    "    model_6.train()\n",
    "    model_7.train()\n",
    "    model_8.train()\n",
    "    model_W.train()\n",
    "        \n",
    "    for data, target in train_loader:\n",
    "\n",
    "        data,target=data.to(device,dtype=torch.float64),target.to(device,dtype=torch.float64)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_1.zero_grad()\n",
    "        optimizer_2.zero_grad()\n",
    "        optimizer_3.zero_grad()\n",
    "        optimizer_4.zero_grad()\n",
    "        optimizer_5.zero_grad()\n",
    "        optimizer_6.zero_grad()\n",
    "        optimizer_7.zero_grad()\n",
    "        optimizer_8.zero_grad()\n",
    "        optimizer_W.zero_grad()\n",
    "            \n",
    "        ################################################################\n",
    "        # Calculation of the final output as a combination of the 8 sub-models\n",
    "        ################################################################\n",
    "            \n",
    "        output_W = model_W(data.double()) \n",
    "        output_1 = model_1(data.double()) \n",
    "        output_2 = model_2(data.double()) \n",
    "        output_3 = model_3(data.double()) \n",
    "        output_4 = model_4(data.double()) \n",
    "        output_5 = model_5(data.double()) \n",
    "        output_6 = model_6(data.double()) \n",
    "        output_7 = model_7(data.double()) \n",
    "        output_8 = model_8(data.double()) \n",
    "            \n",
    "        # Weighted average\n",
    "        \n",
    "        W1 = output_W[:,0,:,:]\n",
    "        W1 = torch.reshape(W1, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W2 = output_W[:,1,:,:]\n",
    "        W2 = torch.reshape(W2, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W3 = output_W[:,2,:,:]\n",
    "        W3 = torch.reshape(W3, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W4 = output_W[:,3,:,:]\n",
    "        W4 = torch.reshape(W4, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W5 = output_W[:,4,:,:]\n",
    "        W5 = torch.reshape(W5, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W6 = output_W[:,5,:,:]\n",
    "        W6 = torch.reshape(W6, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W7 = output_W[:,6,:,:]\n",
    "        W7 = torch.reshape(W7, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W8 = output_W[:,7,:,:]\n",
    "        W8 = torch.reshape(W8, (batch_size, 1,178, 358))\n",
    "        \n",
    "        output_1 = torch.mul(output_1, W1)\n",
    "        output_2 = torch.mul(output_2, W2)\n",
    "        output_3 = torch.mul(output_3, W3)\n",
    "        output_4 = torch.mul(output_4, W4)\n",
    "        output_5 = torch.mul(output_5, W5)\n",
    "        output_6 = torch.mul(output_6, W6)\n",
    "        output_7 = torch.mul(output_7, W7)\n",
    "        output_8 = torch.mul(output_8, W8)\n",
    "            \n",
    "        # Concatenation\n",
    "        concat = torch.cat((output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8), 1) \n",
    "            \n",
    "        # Sum on the last dimension\n",
    "        output = torch.sum(concat, 1)\n",
    "        output = torch.reshape(output, (batch_size, 1,178, 358)) # Reshape to apply the continental mask\n",
    "            \n",
    "        ######################################################\n",
    "        # Loss computation\n",
    "        ######################################################\n",
    "        \n",
    "        compteur = compteur + 1\n",
    "        \n",
    "        output2 = output\n",
    "        target2 = target\n",
    "        \n",
    "        where_are_NaNs = np.isnan(target.cpu())\n",
    "        \n",
    "        ######################\n",
    "        ########################################################### \n",
    "        #  Training only on 50N / 50S\n",
    "        \n",
    "        where_are_NaNs[:,0,0:39,:] = 1\n",
    "        where_are_NaNs[:,0,139:178,:] = 1\n",
    "        \n",
    "        ########################################################### \n",
    "        ######################\n",
    "        \n",
    "        \n",
    "        where_are_NaNs = where_are_NaNs.bool()\n",
    "        where_are_NaNs = ~where_are_NaNs\n",
    "        where_are_NaNs = where_are_NaNs.to(device)\n",
    "        \n",
    "        output2 = torch.masked_select(output2, where_are_NaNs)\n",
    "        target2 = torch.masked_select(target2, where_are_NaNs)\n",
    "                 \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output2.double(), target2.double())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_1.step()\n",
    "        optimizer_2.step()\n",
    "        optimizer_3.step()\n",
    "        optimizer_4.step()\n",
    "        optimizer_5.step()\n",
    "        optimizer_6.step()\n",
    "        optimizer_7.step()\n",
    "        optimizer_8.step()\n",
    "        optimizer_W.step()\n",
    "        \n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "           \n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "    scheduler3.step()\n",
    "    scheduler4.step()\n",
    "    scheduler5.step()\n",
    "    scheduler6.step()\n",
    "    scheduler7.step()\n",
    "    scheduler8.step()\n",
    "    schedulerW.step()\n",
    "    \n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "    model_4.eval()\n",
    "    model_5.eval()\n",
    "    model_6.eval()\n",
    "    model_7.eval()\n",
    "    model_8.eval()\n",
    "    model_W.eval()\n",
    "    \n",
    "    for data, target in valid_loader_1:\n",
    "\n",
    "        data,target=data.to(device,dtype=torch.float),target.to(device,dtype=torch.float)\n",
    "            \n",
    "        output_W = model_W(data.double()) \n",
    "        output_1 = model_1(data.double()) \n",
    "        output_2 = model_2(data.double()) \n",
    "        output_3 = model_3(data.double()) \n",
    "        output_4 = model_4(data.double())\n",
    "        output_5 = model_5(data.double()) \n",
    "        output_6 = model_6(data.double()) \n",
    "        output_7 = model_7(data.double())\n",
    "        output_8 = model_8(data.double())\n",
    "            \n",
    "        \n",
    "        W1 = output_W[:,0,:,:]\n",
    "        W1 = torch.reshape(W1, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W2 = output_W[:,1,:,:]\n",
    "        W2 = torch.reshape(W2, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W3 = output_W[:,2,:,:]\n",
    "        W3 = torch.reshape(W3, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W4 = output_W[:,3,:,:]\n",
    "        W4 = torch.reshape(W4, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W5 = output_W[:,4,:,:]\n",
    "        W5 = torch.reshape(W5, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W6 = output_W[:,5,:,:]\n",
    "        W6 = torch.reshape(W6, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W7 = output_W[:,6,:,:]\n",
    "        W7 = torch.reshape(W7, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W8 = output_W[:,7,:,:]\n",
    "        W8 = torch.reshape(W8, (batch_size, 1,178, 358))\n",
    "        \n",
    "        output_1 = torch.mul(output_1, W1)\n",
    "        output_2 = torch.mul(output_2, W2)\n",
    "        output_3 = torch.mul(output_3, W3)\n",
    "        output_4 = torch.mul(output_4, W4)\n",
    "        output_5 = torch.mul(output_5, W5)\n",
    "        output_6 = torch.mul(output_6, W6)\n",
    "        output_7 = torch.mul(output_7, W7)\n",
    "        output_8 = torch.mul(output_8, W8)\n",
    "\n",
    "        concat = torch.cat((output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8), 1)   \n",
    "            \n",
    "        output = torch.sum(concat, 1)\n",
    "        output = torch.reshape(output, (batch_size, 1,178, 358)) \n",
    "       \n",
    "          \n",
    "        output2 = output\n",
    "        target2 = target\n",
    "        \n",
    "        where_are_NaNs = np.isnan(target.cpu())\n",
    "\n",
    "        \n",
    "        where_are_NaNs[:,0,0:39,:] = 1\n",
    "        where_are_NaNs[:,0,139:178,:] = 1\n",
    "        \n",
    "        \n",
    "        where_are_NaNs = where_are_NaNs.bool()\n",
    "        where_are_NaNs = ~where_are_NaNs\n",
    "        where_are_NaNs = where_are_NaNs.to(device)\n",
    "        \n",
    "        output2 = torch.masked_select(output2, where_are_NaNs)\n",
    "        target2 = torch.masked_select(target2, where_are_NaNs)\n",
    "        \n",
    "\n",
    "            # calculate the batch loss\n",
    "        loss = criterion(output2.double(), target2.double())\n",
    "            # update average validation loss \n",
    "        valid_loss_1 += loss.item()\n",
    "        \n",
    "    \n",
    "    ######################    \n",
    "    # validate the model 2 #\n",
    "    ######################\n",
    "    \n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "    model_4.eval()\n",
    "    model_5.eval()\n",
    "    model_6.eval()\n",
    "    model_7.eval()\n",
    "    model_8.eval()\n",
    "    model_W.eval()\n",
    "    \n",
    "    for data, target in valid_loader_2:\n",
    "\n",
    "        data,target=data.to(device,dtype=torch.float),target.to(device,dtype=torch.float)\n",
    "            \n",
    "        ################################################################\n",
    "        # Calculation of the final output as a combination of the 8 sub-models\n",
    "        ################################################################\n",
    "            \n",
    "        output_W = model_W(data.double())\n",
    "        output_1 = model_1(data.double()) \n",
    "        output_2 = model_2(data.double())\n",
    "        output_3 = model_3(data.double()) \n",
    "        output_4 = model_4(data.double()) \n",
    "        output_5 = model_5(data.double()) \n",
    "        output_6 = model_6(data.double()) \n",
    "        output_7 = model_7(data.double())\n",
    "        output_8 = model_8(data.double()) \n",
    "            \n",
    "        # Weighted average\n",
    "        \n",
    "        W1 = output_W[:,0,:,:]\n",
    "        W1 = torch.reshape(W1, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W2 = output_W[:,1,:,:]\n",
    "        W2 = torch.reshape(W2, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W3 = output_W[:,2,:,:]\n",
    "        W3 = torch.reshape(W3, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W4 = output_W[:,3,:,:]\n",
    "        W4 = torch.reshape(W4, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W5 = output_W[:,4,:,:]\n",
    "        W5 = torch.reshape(W5, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W6 = output_W[:,5,:,:]\n",
    "        W6 = torch.reshape(W6, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W7 = output_W[:,6,:,:]\n",
    "        W7 = torch.reshape(W7, (batch_size, 1,178, 358))\n",
    "        \n",
    "        W8 = output_W[:,7,:,:]\n",
    "        W8 = torch.reshape(W8, (batch_size, 1,178, 358))\n",
    "        \n",
    "        output_1 = torch.mul(output_1, W1)\n",
    "        output_2 = torch.mul(output_2, W2)\n",
    "        output_3 = torch.mul(output_3, W3)\n",
    "        output_4 = torch.mul(output_4, W4)\n",
    "        output_5 = torch.mul(output_5, W5)\n",
    "        output_6 = torch.mul(output_6, W6)\n",
    "        output_7 = torch.mul(output_7, W7)\n",
    "        output_8 = torch.mul(output_8, W8)\n",
    "            \n",
    "        # Concatenation\n",
    "        concat = torch.cat((output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8), 1)  \n",
    "            \n",
    "        # Sum on the last dimension\n",
    "        output = torch.reshape(output, (batch_size, 1,178, 358)) \n",
    "       \n",
    "        #########################       \n",
    "  \n",
    "        output2 = output\n",
    "        target2 = target\n",
    "        \n",
    "        where_are_NaNs = np.isnan(target.cpu())\n",
    "\n",
    "        ######################\n",
    "        ########################################################### \n",
    "        #  Compute the loss only on 50N / 50S\n",
    "        \n",
    "        where_are_NaNs[:,0,0:39,:] = 1\n",
    "        where_are_NaNs[:,0,139:178,:] = 1\n",
    "        \n",
    "        ########################################################### \n",
    "        ######################\n",
    "       \n",
    "        where_are_NaNs = where_are_NaNs.bool()\n",
    "        where_are_NaNs = ~where_are_NaNs\n",
    "        where_are_NaNs = where_are_NaNs.to(device)\n",
    "        \n",
    "        output2 = torch.masked_select(output2, where_are_NaNs) \n",
    "        target2 = torch.masked_select(target2, where_are_NaNs)\n",
    "\n",
    "            # calculate the batch loss\n",
    "        loss = criterion(output2.double(), target2.double())\n",
    "            # update average validation loss \n",
    "        valid_loss_2 += loss.item()\n",
    "\n",
    "        # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.sampler)\n",
    "    valid_loss_1 = valid_loss_1/len(valid_loader_1.sampler)\n",
    "    valid_loss_2 = valid_loss_2/len(valid_loader_2.sampler)\n",
    "    \n",
    "    train_losses_save.append(train_loss)\n",
    "    test_losses_save_1.append(valid_loss_1)\n",
    "    test_losses_save_2.append(valid_loss_2)\n",
    "    \n",
    "    # Save the loss in files\n",
    "    np.save(folder_name2 + 'train_losses_save.npy',train_losses_save)\n",
    "    np.save(folder_name2 + 'test_losses_save_1.npy',test_losses_save_1)\n",
    "    np.save(folder_name2 + 'test_losses_save_2.npy',test_losses_save_2)\n",
    "    \n",
    "    # print loss/training curves to monitor overfitting / convergence\n",
    "    if loss_fig ==1 and epoch%loss_fig_epoque_affichage ==0 :\n",
    "        fig1=plt.figure(figsize=(3,3))\n",
    "        plt.plot(test_losses_save_2, label='Validation loss 2',color='green')\n",
    "        plt.plot(test_losses_save_1, label='Validation loss 1',color='orange')\n",
    "        plt.plot(train_losses_save, label='Training loss',color='blue', linestyle='dashed')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.show()\n",
    "        \n",
    "        # print training/validation statistics \n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss 1: {:.6f} \\tValidation Loss 2: {:.6f}'.format(epoch, n_epochs, train_loss, valid_loss_1, valid_loss_2))\n",
    "    if scatter_corr == 1 and epoch%corr_epoque_affichage ==0 :\n",
    "        print('Epoch: {}/{} \\t Corr coeff 1: {}, RMSE 1: {}      ///         Corr coeff 2: {}, RMSE 2: {}'.format(epoch, n_epochs, round(corcoef1,2), round(rmse1,2),round(corcoef2,2), round(rmse2,2)))\n",
    "\n",
    "        tps_t = time.clock()\n",
    "        print('Temps coul : {} min'.format(round((tps_t - tps_ini)/60),2))\n",
    "        \n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss_1 <= valid_loss_min_1:\n",
    "        torch.save(model_1.state_dict(), model_save_1 )   \n",
    "        torch.save(model_2.state_dict(), model_save_2 ) \n",
    "        torch.save(model_3.state_dict(), model_save_3 ) \n",
    "        torch.save(model_4.state_dict(), model_save_4 ) \n",
    "        torch.save(model_5.state_dict(), model_save_5 ) \n",
    "        torch.save(model_6.state_dict(), model_save_6 ) \n",
    "        torch.save(model_7.state_dict(), model_save_7 ) \n",
    "        torch.save(model_8.state_dict(), model_save_8 ) \n",
    "        torch.save(model_W.state_dict(), model_save_W ) \n",
    "        \n",
    "        valid_loss_min_1 = valid_loss_1\n",
    "        \n",
    "    tracker.epoch_end()\n",
    "tracker.stop()\n",
    "\n",
    "\n",
    "tps_final = time.clock()\n",
    "tps_final_min = round((tps_final - tps_ini)/60,2)\n",
    "print('Time computation : {} min'.format(tps_final_min))\n",
    "\n",
    "    ########################################################################\n",
    "    ### Check overfitting / save loss\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "plt.plot(test_losses_save_2, label='Validation loss 2',color='green')\n",
    "plt.plot(test_losses_save_1, label='Validation loss 1',color='orange')\n",
    "plt.plot(train_losses_save, label='Training loss',color='blue', linestyle='dashed')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig(folder_name2 + 'Loss', dpi= 100,bbox_inches = \"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mon env Python3 Joana",
   "language": "python",
   "name": "mon_env_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
